{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trilateration: Measurement Preparation\n",
    "\n",
    "Given a set of Atlas anchors, downloads anchoring measurements and generates a dataframe containing {probe, anchor, time, RTT} samples, joined to latitude and longitude information about the probe and the anchor. Trilateration and correlation experiments then follow from this prepared data.\n",
    "\n",
    "### Needful things\n",
    "\n",
    "First, imports, utility functions, etc. required for data preparation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil.parser as dp\n",
    "import requests\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "DATA_CACHE_PATH=\"data_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download static files to data cache\n",
    "\n",
    "The probe database is best loaded from a file retrieved periodically. Download the file with `curl` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl http://ftp.ripe.net/ripe/atlas/probes/archive/meta-latest | bunzip2 > data_cache/all_probes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe Metadata\n",
    "\n",
    "Get address and location information for all probes, indexed by probe ID, from the static all_probes.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AtlasProbe = namedtuple(\"AtlasProbe\",\n",
    "           (\"pid\", \"version\", \"nat\", \"ip4\", \"ip6\", \"asn4\", \"asn6\", \"cc\", \"lon\", \"lat\"))\n",
    "\n",
    "def extract_atlas_probe(pobj):\n",
    "    \n",
    "    if \"address_v4\" in pobj and pobj['address_v4'] is not None:\n",
    "        ip4 = pobj[\"address_v4\"]\n",
    "    elif \"prefix_v4\" in pobj:\n",
    "        ip4 = pobj[\"prefix_v4\"]\n",
    "    else:\n",
    "        ip4 = None\n",
    "\n",
    "    if \"address_v6\" in pobj and pobj['address_v6'] is not None:\n",
    "        ip6 = pobj[\"address_v6\"]\n",
    "    elif \"prefix_v4\" in pobj:\n",
    "        ip6 = pobj[\"prefix_v6\"]\n",
    "    else:\n",
    "        ip6 = None\n",
    "\n",
    "    if \"asn_v4\" in pobj:\n",
    "        asn4 = pobj[\"asn_v4\"]\n",
    "    else:\n",
    "        asn4 = None\n",
    "\n",
    "    if \"asn_v6\" in pobj:\n",
    "        asn6 = pobj[\"asn_v6\"]\n",
    "    else:\n",
    "        asn6 = None\n",
    "\n",
    "    if \"tags\" in pobj:\n",
    "        if len(pobj['tags']) > 0 and isinstance(pobj['tags'][0], dict):\n",
    "            alltags = [tag['slug'] for tag in pobj['tags']]\n",
    "        else:\n",
    "            alltags = pobj['tags']\n",
    "\n",
    "        if \"system-v1\" in alltags:\n",
    "            version = 1\n",
    "        elif \"system-v2\" in alltags:\n",
    "            version = 2\n",
    "        elif \"system-v3\" in alltags:\n",
    "            version = 3\n",
    "        elif \"system-anchor\" in alltags:\n",
    "            version = 4\n",
    "        else:\n",
    "            version = 0\n",
    "\n",
    "        nat = \"nat\" in alltags\n",
    "    else:\n",
    "        version = None\n",
    "        nat = None\n",
    "        \n",
    "    # Short circuit: never connected means don't load\n",
    "    if \"status\" in pobj and pobj['status'] == 0:\n",
    "        version = 0\n",
    "    \n",
    "    if \"geometry\" in pobj and \"coordinates\" in pobj['geometry']:\n",
    "        (lon, lat) = pobj['geometry']['coordinates']\n",
    "    elif \"longitude\" in pobj and \"latitude\" in pobj:\n",
    "        lon = pobj['longitude']\n",
    "        lat = pobj['latitude']\n",
    "    else:\n",
    "        lon = None\n",
    "        lat = None\n",
    "\n",
    "    return AtlasProbe(pobj[\"id\"], version, nat, ip4, ip6, asn4, asn6,\n",
    "                      pobj[\"country_code\"], lon, lat)\n",
    "\n",
    "\n",
    "\n",
    "def probe_dataframe_from_file(filename):\n",
    "    data = []\n",
    "    \n",
    "    # make a giant array\n",
    "    with open(filename) as stream:\n",
    "        all_probes = json.loads(stream.read())\n",
    "        for pobj in all_probes[\"objects\"]:\n",
    "            data.append(extract_atlas_probe(pobj))\n",
    "\n",
    "    # create a dataframe from it\n",
    "    df = pd.DataFrame(data, columns=AtlasProbe._fields)\n",
    "    \n",
    "    # indexed by probe ID\n",
    "    df.index = df['pid']\n",
    "    del(df['pid'])\n",
    "    \n",
    "    # and return it\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probe_df = probe_dataframe_from_file(\"data_cache/all_probes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Metadata\n",
    "\n",
    "The v2 API allows us to look up information about anchors by ID. These routines allow cached access to anchor metadata records, and generate a dataframe of all anchor metadata based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AtlasAnchor = namedtuple(\"AtlasAnchor\",\n",
    "           (\"aid\", \"name\", \"pid\", \"ip4\", \"ip6\", \"asn4\", \"asn6\", \"cc\", \"lon\", \"lat\"))\n",
    "\n",
    "def extract_atlas_anchor(aobj):\n",
    "    \n",
    "    if \"id\" in aobj:\n",
    "        aid = int(aobj[\"id\"])\n",
    "    else:\n",
    "        aid = None\n",
    "\n",
    "    if \"fqdn\" in aobj:\n",
    "        name = aobj[\"fqdn\"]\n",
    "    else:\n",
    "        name = None\n",
    "    \n",
    "    if name is not None and name.endswith(\".anchors.atlas.ripe.net\"):\n",
    "        name = name[:-23]\n",
    "        \n",
    "    if \"probe\" in aobj:\n",
    "        pid = int(aobj[\"probe\"])\n",
    "    else:\n",
    "        pid = None\n",
    "    \n",
    "    if \"ip_v4\" in aobj:\n",
    "        ip4 = aobj[\"ip_v4\"]\n",
    "    else:\n",
    "        ip4 = None\n",
    "    \n",
    "    if \"ip_v6\" in aobj:\n",
    "        ip6 = aobj[\"ip_v6\"]\n",
    "    else:\n",
    "        ip6 = None\n",
    "\n",
    "    if \"as_v4\" in aobj and aobj['as_v4'] is not None:\n",
    "        asn4 = int(aobj[\"as_v4\"])\n",
    "    else:\n",
    "        asn4 = None\n",
    "\n",
    "    if \"as_v6\" in aobj and aobj['as_v6'] is not None:\n",
    "        asn6 = int(aobj[\"as_v6\"])\n",
    "    else:\n",
    "        asn6 = None\n",
    "        \n",
    "    if \"country\" in aobj:\n",
    "        cc = aobj['country']\n",
    "    else:\n",
    "        cc = None\n",
    "    \n",
    "    if \"geometry\" in aobj and \"coordinates\" in aobj['geometry']:\n",
    "        (lon, lat) = aobj['geometry']['coordinates']\n",
    "    elif \"longitude\" in aobj and \"latitude\" in aobj:\n",
    "        lon = aobj['longitude']\n",
    "        lat = aobj['latitude']\n",
    "    else:\n",
    "        lon = None\n",
    "        lat = None\n",
    "\n",
    "    return AtlasAnchor(aid, name, pid, ip4, ip6, asn4, asn6, cc, lon, lat)\n",
    "\n",
    "def anchor_dataframe_from_v2api():\n",
    "    data = []\n",
    "    url = \"https://atlas.ripe.net/api/v2/anchors/\"\n",
    "\n",
    "    # iterate over API pagination\n",
    "    while url is not None:\n",
    "        res = requests.get(url)\n",
    "        if not res.ok:\n",
    "            raise RuntimeError(\"Atlas probe API request failed: \"+repr(res.json()))\n",
    "\n",
    "        api_content = json.loads(res.content.decode(\"utf-8\"))\n",
    "        url = api_content['next']\n",
    "        for aobj in api_content[\"results\"]:\n",
    "            data.append(extract_atlas_anchor(aobj))\n",
    "            \n",
    "    # create a dataframe from it\n",
    "    df = pd.DataFrame(data, columns=AtlasAnchor._fields)\n",
    "    \n",
    "    # indexed by probe ID\n",
    "    df.index = df['aid']\n",
    "    del(df['aid'])\n",
    "    \n",
    "    # and return it\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_df = anchor_dataframe_from_v2api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Anchoring Measurement Search\n",
    "\n",
    "Get metadata about available anchoring measurements from the Atlas API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AnchoringMetadata = namedtuple(\"AnchoringMetadata\", (\"aid\", \"type\", \"msm\", \"af\", \"proto\", \"start\", \"stop\", \"probe_ct\"))\n",
    "\n",
    "def anchoring_measurements_from_v2api(how_many = None):\n",
    "    data = []\n",
    "    url = \"https://atlas.ripe.net/api/v2/anchor-measurements/?include=measurement\"\n",
    "\n",
    "    # iterate over API pagination\n",
    "    while url is not None:\n",
    "        res = requests.get(url)\n",
    "        if not res.ok:\n",
    "            raise RuntimeError(\"Atlas probe API request failed: \"+repr(res.json()))\n",
    "\n",
    "        api_content = json.loads(res.content.decode(\"utf-8\"))\n",
    "        url = api_content['next']\n",
    "        for mobj in api_content[\"results\"]:\n",
    "            try:\n",
    "                aid = int(mobj[\"target\"].strip(\"/\").split(\"/\")[-1])\n",
    "                typ = mobj[\"type\"]\n",
    "                msm = int(mobj[\"measurement\"][\"id\"])\n",
    "                af = int(mobj[\"measurement\"][\"af\"])\n",
    "                if \"protocol\" in mobj[\"measurement\"]:\n",
    "                    proto = mobj[\"measurement\"][\"protocol\"]\n",
    "                elif typ == \"ping\":\n",
    "                    proto = \"ICMP\"\n",
    "                else:\n",
    "                    proto = None\n",
    "                start = mobj[\"measurement\"][\"start_time\"]\n",
    "                stop = mobj[\"measurement\"][\"stop_time\"]\n",
    "                probe_ct = mobj[\"measurement\"][\"participant_count\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "            data.append(AnchoringMetadata(aid, typ, msm, af, proto, start, stop, probe_ct))\n",
    "        \n",
    "        if how_many is not None and len(data) >= how_many:\n",
    "            break\n",
    "            \n",
    "    # create a dataframe from it\n",
    "    df = pd.DataFrame(data, columns=AnchoringMetadata._fields)\n",
    "    \n",
    "    # indexed by MSM ID\n",
    "    df.index = df['msm']\n",
    "    del(df['msm'])\n",
    "    \n",
    "    # and return it\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Selection\n",
    "\n",
    "So now we have a list of all anchoring MSMs. Select a set of anchors by name, and derive IPv4 and IPv4 ping measurements from the anchoring measurements toward those anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANCHOR_NAMES_WE_LIKE = [\n",
    "    \"ar-bue-as4270\",   # Buenos Aires, Argentina\n",
    "    \"at-vie-as1120\",   # Vienna, Austria\n",
    "    \"au-mel-as38796\",  # Melbourne, Austraila\n",
    "    \"bd-dac-as24122\",  # Dacca, Bangladesh\n",
    "    \"bg-sof-as8866\",   # Sofia, Bulgaria\n",
    "    \"ca-mtr-as852\",    # Montreal, Canada\n",
    "    \"ch-zrh-as559\",    # Zurich, Switzerland\n",
    "    \"de-fra-as8763\",   # Frankfurt, Germany\n",
    "    \"de-ham-as201709\", # Hamburg, Germany\n",
    "    \"de-muc-as5539\",   # Munich, Germany\n",
    "    \"ee-tll-as51349\",  # Talinn, Estonia\n",
    "    \"es-bcn-as13041\",  # Barcelona, Spain\n",
    "    \"fr-par-as1307\",   # Paris, France\n",
    "    \"gr-ath-as5408\",   # Athens, Greece\n",
    "    \"hk-hkg-as43996\",  # Hong Kong SAR, China\n",
    "    \"hu-bud-as12303\",  # Budapest, Hungary\n",
    "    \"id-jkt-as10208\",  # Jakarta, Indonesia\n",
    "    \"ie-dub-as1213\",   # Dublin, Ireland\n",
    "    \"in-bom-as33480\",  # Mumbai, India\n",
    "    \"it-trn-as12779\",  # Turin, Italy\n",
    "    \"jp-tyo-as2500\",   # Tokyo, Japan\n",
    "    \"kz-ala-as21299\",  # Almaty, Kazakhstan\n",
    "    \"nl-ams-as3333\",   # Amsterdam, Holland\n",
    "    \"nz-wlg-as9834\",   # Wellington, New Zealand\n",
    "    \"qa-doh-as8781\",   # Doha, Qatar\n",
    "    \"ru-mow-as15835\",  # Moscow, Russia\n",
    "    \"se-sto-as8674\",   # Stockholm, Sweden\n",
    "    \"uk-lon-as5607\",   # London, England\n",
    "    \"us-dal-as2914\",   # Dallas, USA\n",
    "    \"us-den-as7922\",   # Denver, USA\n",
    "    \"us-mia-as33280\",  # Miami, USA\n",
    "    \"us-sjc-as22300\",  # San Jose, USA\n",
    "]\n",
    "\n",
    "aid_by_name = anchor_df.loc[:,('name',)]\n",
    "aid_by_name['aid'] = aid_by_name.index\n",
    "aid_by_name.index = aid_by_name['name']\n",
    "del aid_by_name['name']\n",
    "\n",
    "ANCHORS_WE_LIKE = [aid_by_name.loc[aname]['aid'] for aname in ANCHOR_NAMES_WE_LIKE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "am_df = anchoring_measurements_from_v2api()\n",
    "am2a_df = am_df[am_df['aid'].isin(ANCHORS_WE_LIKE)]\n",
    "ping2a_df = am2a_df[am2a_df['type'] == 'ping']\n",
    "\n",
    "MSMS_WE_LIKE = ping2a_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the [paper](paper.ipynb) we select six hours starting at noon UTC 11 July 2017 (the author's birthday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_TIME = int(dp.parse(\"2017-07-11T12:00:00Z\").timestamp())\n",
    "STOP_TIME = int(dp.parse(\"2017-07-11T18:00:00Z\").timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick checkpoint here\n",
    "MSMS_WE_LIKE = [1026364, 1026366, 1026392, 1026394, 1026400, 1026402, 1042256,\n",
    "       1042258, 1043287, 1043289, 1404334, 1404336, 1423187, 1423189,\n",
    "       1437285, 1437287, 1446418, 1446420, 1583041, 1583043, 1589863,\n",
    "       1589865, 1591157, 1591159, 1664874, 1664876, 1665837, 1665839,\n",
    "       1668852, 1668854, 1768006, 1768008, 1769992, 1769994, 1790233,\n",
    "       1790235, 1849606, 1849608, 1990234, 1990236, 2055769, 2055771,\n",
    "       2096535, 2096537, 2395061, 2395063, 2398551, 2398553, 2417651,\n",
    "       2417653, 3295750, 3295764, 3315654, 3315657, 3614642, 3614645,\n",
    "       3622419, 3622422, 6969365, 6969368, 7861647, 7861650, 8434916,\n",
    "       8434919, 9180599, 9180602, 9180619, 9180622, 9180653, 9180656,\n",
    "       9180667, 9180670, 9180705, 9180708, 9180725, 9180728, 9180767,\n",
    "       9180770, 9180929, 9180932, 9180955, 9180958, 9180990, 9180994,\n",
    "       9181094, 9181097, 9181100, 9181103, 9181175, 9181178, 9181254,\n",
    "       9181257, 9181266, 9181269, 9181276, 9181279, 9181282, 9181285,\n",
    "       9181297, 9181300, 9181317, 9181320, 9181352, 9181355, 9181364,\n",
    "       9181367, 9181397, 9181400, 9181529, 9181532, 9181693, 9181696,\n",
    "       9181746, 9181749, 9181778, 9181781, 9181821, 9181824, 9183385,\n",
    "       9183388, 9183524, 9183527, 9183541, 9183544, 9183659, 9183662,\n",
    "       9183787, 9183790]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RTT Sample Extraction\n",
    "\n",
    "Given a set of MSMs, download (or read from cache) and generate a set of RTT tuples:\n",
    "\n",
    "- time: timestamp of the measurement underlying the sample\n",
    "- aid: Atlas probe ID of anchor\n",
    "- pid: Atlas probe ID of probe\n",
    "- af: address family used (4 or 6)\n",
    "- proto: string identifier of protocol used\n",
    "- rtt: RTT sample in microseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Alp = namedtuple(\"Alp\", (\"time\",\"af\",\"proto\",\"pid\",\"sip\",\"dip\",\"rtt\"))\n",
    "\n",
    "RTT_NONE = 0.0\n",
    "DATA_CACHE_PATH = 'data_cache'\n",
    "\n",
    "def gen_dict(msm_ary):\n",
    "    for a_res in msm_ary:\n",
    "        yield a_res\n",
    "\n",
    "def gen_alp(msm_ary):\n",
    "    for a_res in msm_ary:\n",
    "        if a_res['type'] == 'ping':\n",
    "            if \"rcvd\" in a_res:\n",
    "                for x in a_res[\"result\"]:\n",
    "                    rtt = None\n",
    "                    try: \n",
    "                        rtt = float(x)\n",
    "                    except:\n",
    "                        try:\n",
    "                            rtt = float(x['rtt'])\n",
    "                        except:\n",
    "                            pass\n",
    "                    if rtt:\n",
    "                        yield Alp(int(a_res['timestamp']), a_res['af'], a_res['proto'], \n",
    "                                  a_res['prb_id'], a_res['src_addr'], a_res['dst_addr'], \n",
    "                                  int(rtt * 1000))\n",
    "        \n",
    "        elif a_res['type'] == 'traceroute':\n",
    "            if ('result' in a_res) and ('result' in a_res['result'][-1]):\n",
    "                for h_res in a_res['result'][-1]['result']:\n",
    "                    if ('from' in h_res) and ('rtt' in h_res) and (h_res['from'] == a_res['dst_addr']):\n",
    "                        yield Alp(int(a_res['timestamp']), a_res['af'], a_res['proto'] + '_TR', \n",
    "                                  a_res['prb_id'], a_res['src_addr'], a_res['dst_addr'], h_res['rtt'])\n",
    "\n",
    "        # For HTTP, return each subresult as a separate RTT sample\n",
    "        elif a_res['type'] == 'http':\n",
    "            for r_res in a_res['result']:\n",
    "                if ('res' in r_res) and (r_res['res'] < 400):\n",
    "                    yield Alp(a_res['timestamp'], r_res['af'], 'HTTP', \n",
    "                              a_res['prb_id'], r_res['src_addr'], r_res['dst_addr'], r_res['rt'])                    \n",
    "\n",
    "        \n",
    "\n",
    "def gen_msm(msm, gen=gen_alp, cachedir=None, start=None, stop=None):\n",
    "    \"\"\"\n",
    "    Given an MSM, fetch it from the cache or from the RIPE Atlas API.\n",
    "    Yield each separate result according to the generation function.\n",
    "    \"\"\"\n",
    "    url = \"https://atlas.ripe.net/api/v2/measurements/%u/results/\" % (msm,)\n",
    "\n",
    "    params = {\"format\": \"json\"}\n",
    "    if start is not None and stop is not None:\n",
    "        params[\"start\"] = str(start)\n",
    "        params[\"stop\"] = str(stop)\n",
    "    \n",
    "    if cachedir and os.path.isdir(cachedir):\n",
    "        filepath = os.path.join(cachedir, \"measurement\", \"%u.json\" % (msm,))\n",
    "\n",
    "        # download if not present\n",
    "        if not os.path.isfile(filepath):\n",
    "            with open(filepath, mode=\"wb\") as file:\n",
    "                print(\"Cache miss, retrieving \"+url)\n",
    "                res = requests.get(url, params=params)\n",
    "\n",
    "                if not res.ok:\n",
    "                    raise \"Atlas measurement API request failed: \"+repr(res.json())\n",
    "                \n",
    "                file.write(res.content)\n",
    "\n",
    "        # then read from cache\n",
    "        with open(filepath) as stream:\n",
    "            yield from gen(json.loads(stream.read()))\n",
    "\n",
    "    else:\n",
    "        # just read from the net\n",
    "        res = requests.get(url, params=params)\n",
    "        yield from gen(json.loads(res.content.decode(\"utf-8\")))\n",
    "\n",
    "def msm_dfgen(msms, cachedir=None, start=None, stop=None):\n",
    "    for msm in msms:\n",
    "        yield from gen_msm(msm, cachedir=cachedir, start=start, stop=stop)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(msm_dfgen(MSMS_WE_LIKE, cachedir=DATA_CACHE_PATH, start=START_TIME, stop=STOP_TIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a target anchor ID to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aid_by_ip4 = anchor_df.loc[:,('ip4',)]\n",
    "aid_by_ip4['aid'] = aid_by_ip4.index\n",
    "aid_by_ip4.index = aid_by_ip4['ip4']\n",
    "del aid_by_ip4['ip4']\n",
    "\n",
    "aid_by_ip6 = anchor_df.loc[:,('ip6',)]\n",
    "aid_by_ip6['aid'] = aid_by_ip6.index\n",
    "aid_by_ip6.index = aid_by_ip6['ip6']\n",
    "del aid_by_ip6['ip6']\n",
    "\n",
    "df = pd.concat((df[df['af']==4].join(aid_by_ip4, on=\"dip\"), df[df['af']==6].join(aid_by_ip6, on=\"dip\"))).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the dataframe in preparation for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['time'] * 1e9)\n",
    "df['aid'] = pd.to_numeric(df['aid'], downcast='unsigned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and write data to HDF5\n",
    "\n",
    "Work continues in the analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore('rtt.h5') as store:\n",
    "    store['anchor_df'] = anchor_df\n",
    "    store['probe_df'] = probe_df\n",
    "    store['rtt_df'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
